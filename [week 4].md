# 4주차

태그: 완료

## 활성화 함수

---

- 활성화 함수로 $\sigma$보다 사용하기에 적합한 비선형 함수가 있다.
- $tanh$의 장점: $a=tanh(z)$이 $a=\sigma(z)$보다 좋다. $tanh$는 값이 -1부터 1 사이이기 때문에 평균값이 0에 더 가까워 **데이터의 중심을 원점으로 이동하게 하는 효과**가 있다. 이는 다음 층에서의 학습을 더 용이하게 한다. 따라서 은닉층에서는 $\sigma$보다 $tanh$를 사용한다. 또한 $tanh$의 기울기가 큰 범위가 넓기 때문에 $sigmoid$ 보다 기울기 소실 문제를 완화할 수 있다. 다만  $y\in {{0,1}}$  이진 분류의 출력층에서는 $\sigma$를 사용한다.
- $tanh$와 $\sigma$의 단점: z가 굉장히 크거나 작으면 함수의 도함수가 굉장히 작아져 경사 하강법이 매우 느려진다.
- $ReLU$의 장점: $tanh$와 $\sigma$의 단점을 극복할 수 있다. 요즘에는 $ReLU$가 가장 많이 쓰인다. **신경망을 빠르게 학습**시킬 수 있다. 학습을 느리게 하는 원인인, 함수의 기울기가 0에 수렴하는 걸 막기 때문이다. z=0일 확률은 매우 작지만, z=0이라도 상관 없다. 만약 z가 음수이고 이 데이터가 중요한 경우에는 반드시 $ReLU$를 고집하지 말고 $tanh$나 $sigmoid$를 써도 상관 없다.
- $ReLU$의 단점: z가 음수일 때 도함수가 0이다. 그러나 은닉 노드의 z는 0보다 크기 때문에 실제로는 잘 동작한다. 예를 들어 집값 예측에서, $\hat{y}$는 가격으로 항상 양수이기 때문에 $ReLU$가 잘 동작한다.
- $leaky ReLU$: z가 음수할 때 도함수가 0이 아니게 되도록 해준다. 밑의 그래프를 자세히 보면 z<0일 때 계수가 0.01이기 때문에 그래프가 살짝 꺾이는 것을 볼 수 있다. 실제로는 많이 쓰이지 않지만 쓰인다면 $ReLU$보다 좋은 결과를 보여준다. 실제로 $leaky ReLU$에서 0.01 말고 다른 값을 쓰면 기능이 급격하게 저하될 수 있다.

![Untitled](https://github.com/user-attachments/assets/5422192f-4410-4830-927b-bd765ed6d24c)

![Untitled 1](https://github.com/user-attachments/assets/388b1334-18b3-4d4c-b5b4-02d911e958b2)

*x축은 z, y축은 a=g(z), g는 활성화 함수*

## 왜 비선형 활성화 함수를 써야 할까요?

---

- $g(z)=z$라는 선형 활성화 함수가 있다고 한다면 아무리 신경망을 거쳐도 $g(g(g(z))))=z$로, 은닉층이 없는 것과 다름 없다. 선형 은닉층은 쓸모 없다. **따라서 은닉층에서 선형식을 사용한다면(비선형식을 쓰지 않는다면) 신경망이 깊어져도 의미 있는 결과를 출력하기 어렵다. 또한 신호의 연결 강도를 조절한다. 그래디언트 소실/폭주 문제를 방지할 수 있다.**
- 선형 활성화 함수를 쓰는 곳은 출력층이다. 은닉층에서는 사용하지 않는다.
- 비선형 활성화 함수에는 $\sigma,ReLU, tanh, leaky ReLU$ 등이 있다.

## 활성화 함수의 미분

---

![Untitled 2](https://github.com/user-attachments/assets/df46369b-f37f-46b7-8553-4cbbc3887c17)

![Untitled 3](https://github.com/user-attachments/assets/0d17c361-bab2-42a2-b6a7-4a23c51ed59e)

- 시그모이드에서, $a=g(z)$이기 때문에 $a$의 값을 안다면 쉽게 $g'(z)$를 구할 수 있다는 이점이 있다. $(\because g’(z)=a(1-a))$
- $tanh$에서, $a=g(z)$이기 때문에 $a$의 값을 안다면 쉽게 $g'(z)$를 구할 수 있다는 이점이 있다. $(\because g'(z)=1-a^{2})$

## 신경망 네트워크와 경사 하강법
![IMG_5105](https://github.com/user-attachments/assets/e7a22e77-f3a3-4e2b-92ad-aa543dad32c9)

---

![IMG_5099](https://github.com/user-attachments/assets/9e2b80c3-5e66-4c14-9589-6baac1e70fad)

## 역전파에 대한 이해

---

- 로지스틱 회귀의 역전파를 구하면 다음과 같다.

![Untitled 4](https://github.com/user-attachments/assets/545e0b18-c0ce-43dd-9aa1-36c7b416d41b)

- $*n^{[0]}, n^{[1]}, n^{[2]}$…는 각 층 별 노드의 개수, m은 $X=[x^{(1)}, x^{(2)}…x^{(m)}]$로, 훈련 데이터 x의 개수*
- 벡터화를 하면 shape가 (#node, m)이다.

## 랜덤 초기화

---

- 신경망에서 w의 초기값을 0으로 설정하고 경사 하강법을 적용할 경우 올바르게 작동하지 않는다.
    - $dw$를 계산했을 때 모든 층이 대칭이 되어 같은 값을 가지게 되기 때문이다→쓸모 없음
    - 그러나 b는 w와 같은 문제가 발생하지 않아 영행렬로 초기화해줘도 문제 없다.
- 따라서 np.random.rand()를 통해 0이 아닌 랜덤값을 부여해줘야 한다.

<aside>
📢 **random 함수**

---

np.random.randint(100)-0~99까지의 정수 하나를 랜덤으로 반환

np.random.rand(100)-0~1사이의 수 100개를 랜덤으로 반환

np.random.randn(100)-가우시안 정규분포를 따르는 수 100개를 랜덤으로 반환

np.random.rand()-0~1사이의 수 하나를 랜덤으로 반환

</aside>

![Untitled 5](https://github.com/user-attachments/assets/82e0132d-c802-4606-82bf-df14b42b84c9)

```python
w^{[1]}=np.random.randn((2,2))*0.01 #(2,2) 행렬
b_{[1]}=np.zeros((2,1)) 
w^{[2]}=np.random.randn((1,2))*0.01
b_{[2]}=np.zeros((1,1))
```

💡w에 0.01을 곱해 w를 매우 작게 만들어주는 이유는, 시그모이드/tanh를 이용했을 때 z가 너무 큰 값이 되면 기울기 이슈로 학습의 속도가 매우 느려진다는 것에 있다. w가 너무 크면 이에 따라 z도 커지고, 활성화 함수를 거친 a도 커지기 때문에 애초에 초기의 w를 작은 값으로 설정해야 할 필요가 있다. 0.01 외의 다른 작은 수를 곱해줘도 상관 없다!

---

## QUIZ

![Untitled 6](https://github.com/user-attachments/assets/f26a2fcf-e3f9-4c89-9f9f-80824d635441)

- break symmetry: 모든 은닉층의 가중치를 동일하게 초기화했을 때, 각 은닉층의 모든 노드가 동일한 함수를 계산하고 동일한 활성값을 출력해 동일한 방식으로 업데이트되는 대칭성 문제를 해결하는 것이다.
- 대칭성이 발생한 경우 학습이 효과적으로 이루어지지 않기 때문에 은닉층의 가중치를 무작위로 초기화해 각 노드가 서로 다른 초기 가중치를 갖도록 해야 한다.
- symmetry 문제는 주로 심층 신경망에서 발생하는 문제이며 로지스틱 회귀와 같은 단일층 모델에서는 발생하지 않는다. → False!
